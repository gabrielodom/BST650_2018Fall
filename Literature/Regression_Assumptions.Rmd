---
title: "Checking Regression Assumptions"
author: "Gabriel Odom"
date: "11/26/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Overview
We have built a regression model assessing the relationship between blood cholesterol and age, weight, and systolic blood pressure.

What are the five assumptions of multiple linear regression?

0. The linear model is appropriate
1. Errors are normally distributed
2. Errors are centred at 0
3. Errors have constant variance (Homoskedasticity)
4. Errors are independent
5. No multicollinearity

We can encapsulate the first four into a single equation:
\[
\varepsilon_i\ \sim \text{i.i.d.}\ N(0, \sigma)
\]



## Setup
We load the packages and data we need:
```{r}
library(tidyverse)
wcgs_df <- read_csv("../data/wcgs.csv")
```

Now we can construct the linear model in order to test the assumptions above.
```{r}
chol_mod <- lm(chol ~ age + weight + sbp, data = wcgs_df)
```



## Assumptions of Multiple Linear Regression

### Assumption 1: Model Errors are Normally Distributed
Overall, the residuals appear to be normally distributed, based on the histogram, but we may have a few outliers. When we inspect the QQ-plot, we probably have an outlier.
```{r}
str(chol_mod)
# Histogram
hist(chol_mod$residuals)

# QQ-Plot
qqnorm(chol_mod$residuals)
qqline(chol_mod$residuals)
```

Try a transformation:
```{r}
hist(wcgs_df$chol)
cholSq_mod <- lm(sqrt(chol) ~ age + weight + sbp, data = wcgs_df)
hist(cholSq_mod$residuals)

qqnorm(cholSq_mod$residuals)
qqline(cholSq_mod$residuals)
```

That's probably not any better. Let's leave the data alone...


### Assumption 2: Model Errors are Centred at 0
The quick and dirty way would be to look at the centre of the histogram. We can also perform a statistical test:
```{r}
t.test(chol_mod$residuals, alternative = "two.sided", mu = 0)
```

0 is dead-set in the middle of the confidence interval. We're rather confident that the mean of the residuals is close enough to 0.


### Assumption 3: Model Errors are Homoskedastic
From StackExchange, I found that I can use the Breusch-Pagan test for constant variance of the model errors. My null hypothesis is that the variance of the errors is constant.
```{r}
library(lmtest)
# Perform the Breusch-Pagan test against heteroskedasticity
res <- bptest(chol_mod)
res
```

Based on this $p$-value, we fail to reject the null hypothesis. Therefore, the model errors appear to be homoskedastic.

Also, we can plot the errors.
```{r}
plot(chol_mod$residuals)
```

The spread of the residuals appears to be constant.

### Assumption 4: Model Errors are Independent
We found on [this Duke website on linear regression assumptions](http://people.duke.edu/~rnau/testing.htm#independence) that we can use the Durbin-Watson test on the model errors. Our null hypothesis is that the auto-correlation of the model errors (with themselves) is 0. Recall that we have shown that our errors are probably normally distributed. Thus, if these errors also have autocorrelation of 0, then they are independent. Symbolically, 
\[
\textbf{x} \sim N(\mu, \sigma)\ \text{AND}\ \rho(x_i, x_j) = 0\ \forall i\ne j \Longrightarrow x_i \perp \!\!\! \perp x_j \forall i \ne j.
\]

We then test that the autocorrelation is equal to 0.
```{r}
dwtest(chol_mod)
```
Because the $p$-value is less than 0.05, we should reject the null hypothesis that the autocorrelation of the model errors equals 0. Therefore the model errors appear to be dependent on each other. This is a problem. We need to inspect this further.

Let's also plot the autocorrelation function.
```{r}
acf(chol_mod$residuals)
```

Values above the blue line indicate statistical significance. However, this significant autocorrelation is less than 5%, so it is most likely not practically significant. Thus, the errors are practically indpendent, but statistically dependent.

### Assumption 5: Model Predictors are not Perfectly Correlated